nohup: ignoring input
Not using distributed mode
Creating dataset...
3335
number of training samples: 3335
number of testing samples: 833
Dataset len: 833
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.embeddings.token_type_embeddings.weight']
- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
number of trainable parameters: 266116703
0/416 loss: 16.37469482421875, loss_lm: 11.397547721862793, loss_cls: 4.977147102355957
10/416 loss: 15.477678298950195, loss_lm: 10.461503028869629, loss_cls: 5.016175746917725
20/416 loss: 15.056196212768555, loss_lm: 10.027944564819336, loss_cls: 5.028251647949219
30/416 loss: 14.03175163269043, loss_lm: 9.081343650817871, loss_cls: 4.9504075050354
40/416 loss: 13.929004669189453, loss_lm: 8.82496166229248, loss_cls: 5.1040425300598145
50/416 loss: 13.307344436645508, loss_lm: 8.457533836364746, loss_cls: 4.84981107711792
60/416 loss: 13.032890319824219, loss_lm: 8.01122760772705, loss_cls: 5.021663188934326
70/416 loss: 12.355452537536621, loss_lm: 7.804624080657959, loss_cls: 4.550828456878662
80/416 loss: 12.639732360839844, loss_lm: 7.95945930480957, loss_cls: 4.680272579193115
90/416 loss: 11.874732971191406, loss_lm: 7.486896991729736, loss_cls: 4.38783597946167
100/416 loss: 11.63748550415039, loss_lm: 7.394295692443848, loss_cls: 4.243189811706543
110/416 loss: 11.6236572265625, loss_lm: 7.379420757293701, loss_cls: 4.244236946105957
120/416 loss: 11.143204689025879, loss_lm: 7.214632511138916, loss_cls: 3.928572416305542
130/416 loss: 10.861891746520996, loss_lm: 7.018462657928467, loss_cls: 3.8434290885925293
140/416 loss: 10.86129379272461, loss_lm: 7.0987162590026855, loss_cls: 3.7625770568847656
150/416 loss: 10.796836853027344, loss_lm: 7.019527435302734, loss_cls: 3.7773094177246094
160/416 loss: 9.966917037963867, loss_lm: 6.480834007263184, loss_cls: 3.4860832691192627
170/416 loss: 10.174779891967773, loss_lm: 6.887424945831299, loss_cls: 3.2873551845550537
180/416 loss: 9.437026977539062, loss_lm: 6.553164958953857, loss_cls: 2.883862018585205
190/416 loss: 9.25973892211914, loss_lm: 6.2651214599609375, loss_cls: 2.994616985321045
200/416 loss: 9.87451171875, loss_lm: 6.589460372924805, loss_cls: 3.2850513458251953
210/416 loss: 8.930486679077148, loss_lm: 6.140608787536621, loss_cls: 2.7898783683776855
220/416 loss: 9.265917778015137, loss_lm: 6.311246871948242, loss_cls: 2.9546706676483154
230/416 loss: 8.315423011779785, loss_lm: 6.051352500915527, loss_cls: 2.2640702724456787
240/416 loss: 7.982202529907227, loss_lm: 5.883615493774414, loss_cls: 2.0985872745513916
250/416 loss: 8.3753023147583, loss_lm: 5.984496116638184, loss_cls: 2.390805959701538
260/416 loss: 7.709012985229492, loss_lm: 5.424556255340576, loss_cls: 2.284456729888916
270/416 loss: 7.415497303009033, loss_lm: 5.401602745056152, loss_cls: 2.013894557952881
280/416 loss: 8.014894485473633, loss_lm: 5.57411527633667, loss_cls: 2.440779447555542
290/416 loss: 8.689806938171387, loss_lm: 5.660128593444824, loss_cls: 3.0296781063079834
300/416 loss: 7.144773483276367, loss_lm: 4.830426216125488, loss_cls: 2.314347505569458
310/416 loss: 7.520267963409424, loss_lm: 4.753450870513916, loss_cls: 2.766817092895508
320/416 loss: 6.934618949890137, loss_lm: 4.757220268249512, loss_cls: 2.177398681640625
330/416 loss: 6.920598030090332, loss_lm: 4.594546318054199, loss_cls: 2.326051712036133
340/416 loss: 6.6089372634887695, loss_lm: 4.5124311447143555, loss_cls: 2.096505880355835
350/416 loss: 6.246654033660889, loss_lm: 4.471384525299072, loss_cls: 1.7752693891525269
360/416 loss: 7.039234161376953, loss_lm: 4.683295726776123, loss_cls: 2.355938673019409
370/416 loss: 6.166097640991211, loss_lm: 4.241831302642822, loss_cls: 1.9242665767669678
380/416 loss: 5.907122611999512, loss_lm: 4.175458908081055, loss_cls: 1.7316638231277466
390/416 loss: 6.6367292404174805, loss_lm: 4.368241310119629, loss_cls: 2.2684881687164307
400/416 loss: 6.653575897216797, loss_lm: 4.224581241607666, loss_cls: 2.4289944171905518
410/416 loss: 5.512173652648926, loss_lm: 3.861326217651367, loss_cls: 1.6508475542068481
0/416 loss: 6.16610860824585, loss_lm: 4.147170543670654, loss_cls: 2.0189380645751953
10/416 loss: 6.458765029907227, loss_lm: 4.285403251647949, loss_cls: 2.1733620166778564
20/416 loss: 6.180427551269531, loss_lm: 3.7854580879211426, loss_cls: 2.3949694633483887
30/416 loss: 5.659732818603516, loss_lm: 3.557913064956665, loss_cls: 2.1018197536468506
40/416 loss: 6.091290473937988, loss_lm: 3.966221809387207, loss_cls: 2.1250689029693604
50/416 loss: 5.809711456298828, loss_lm: 3.7713592052459717, loss_cls: 2.0383524894714355
60/416 loss: 5.418396472930908, loss_lm: 3.6661219596862793, loss_cls: 1.752274513244629
70/416 loss: 6.366522312164307, loss_lm: 3.8438968658447266, loss_cls: 2.52262544631958
80/416 loss: 4.601469039916992, loss_lm: 3.0663299560546875, loss_cls: 1.5351393222808838
90/416 loss: 4.999108791351318, loss_lm: 3.2983579635620117, loss_cls: 1.7007509469985962
100/416 loss: 4.894205570220947, loss_lm: 3.2731919288635254, loss_cls: 1.6210135221481323
110/416 loss: 5.06229829788208, loss_lm: 3.2310173511505127, loss_cls: 1.831281065940857
120/416 loss: 5.011081218719482, loss_lm: 3.197538137435913, loss_cls: 1.8135429620742798
130/416 loss: 5.414764404296875, loss_lm: 3.2871599197387695, loss_cls: 2.1276042461395264
140/416 loss: 5.312705993652344, loss_lm: 3.41770339012146, loss_cls: 1.8950027227401733
150/416 loss: 5.090546607971191, loss_lm: 3.2191989421844482, loss_cls: 1.8713475465774536
160/416 loss: 4.596503257751465, loss_lm: 2.844470500946045, loss_cls: 1.7520325183868408
170/416 loss: 5.402619361877441, loss_lm: 3.297330856323242, loss_cls: 2.105288505554199
180/416 loss: 5.675042152404785, loss_lm: 3.330714464187622, loss_cls: 2.344327926635742
190/416 loss: 5.49378776550293, loss_lm: 3.3404088020324707, loss_cls: 2.15337872505188
200/416 loss: 5.843931198120117, loss_lm: 3.4592647552490234, loss_cls: 2.3846662044525146
210/416 loss: 5.99955940246582, loss_lm: 3.2410268783569336, loss_cls: 2.758532762527466
220/416 loss: 5.493400573730469, loss_lm: 3.2051358222961426, loss_cls: 2.288264513015747
230/416 loss: 5.237393379211426, loss_lm: 2.9347047805786133, loss_cls: 2.3026885986328125
240/416 loss: 5.217502117156982, loss_lm: 2.97000789642334, loss_cls: 2.2474942207336426
250/416 loss: 5.230358123779297, loss_lm: 3.212996482849121, loss_cls: 2.0173614025115967
260/416 loss: 5.773333549499512, loss_lm: 3.310215473175049, loss_cls: 2.463118314743042
270/416 loss: 4.7926716804504395, loss_lm: 2.992231607437134, loss_cls: 1.8004399538040161
280/416 loss: 5.228549957275391, loss_lm: 2.9366023540496826, loss_cls: 2.291947603225708
290/416 loss: 4.578465461730957, loss_lm: 2.798308849334717, loss_cls: 1.7801563739776611
300/416 loss: 4.878100872039795, loss_lm: 2.903107166290283, loss_cls: 1.9749937057495117
310/416 loss: 6.049097537994385, loss_lm: 3.531561851501465, loss_cls: 2.51753568649292
320/416 loss: 4.814369201660156, loss_lm: 3.0350451469421387, loss_cls: 1.779323935508728
330/416 loss: 4.6912431716918945, loss_lm: 3.0015664100646973, loss_cls: 1.6896770000457764
340/416 loss: 4.930588722229004, loss_lm: 2.795362710952759, loss_cls: 2.135226011276245
350/416 loss: 4.4043378829956055, loss_lm: 2.6331470012664795, loss_cls: 1.771190881729126
360/416 loss: 4.847611427307129, loss_lm: 2.7581777572631836, loss_cls: 2.0894339084625244
370/416 loss: 4.610360145568848, loss_lm: 2.7206287384033203, loss_cls: 1.8897311687469482
380/416 loss: 5.098444938659668, loss_lm: 2.7500052452087402, loss_cls: 2.3484396934509277
390/416 loss: 4.644501686096191, loss_lm: 2.736837863922119, loss_cls: 1.9076639413833618
400/416 loss: 4.78333044052124, loss_lm: 2.6992321014404297, loss_cls: 2.0840983390808105
410/416 loss: 5.071775436401367, loss_lm: 3.120966911315918, loss_cls: 1.9508085250854492
0/416 loss: 4.5921125411987305, loss_lm: 2.6688718795776367, loss_cls: 1.9232407808303833
10/416 loss: 5.366726875305176, loss_lm: 2.7204623222351074, loss_cls: 2.6462643146514893
20/416 loss: 4.8978776931762695, loss_lm: 2.6265902519226074, loss_cls: 2.271287679672241
30/416 loss: 4.521836280822754, loss_lm: 2.731961488723755, loss_cls: 1.789874792098999
40/416 loss: 4.797069072723389, loss_lm: 2.7050981521606445, loss_cls: 2.091970920562744
50/416 loss: 4.5429840087890625, loss_lm: 2.3053998947143555, loss_cls: 2.237584352493286
60/416 loss: 5.365924835205078, loss_lm: 2.8223586082458496, loss_cls: 2.5435664653778076
70/416 loss: 4.503232479095459, loss_lm: 2.7023115158081055, loss_cls: 1.800920844078064
80/416 loss: 4.1247406005859375, loss_lm: 2.400806188583374, loss_cls: 1.7239344120025635
90/416 loss: 4.553879737854004, loss_lm: 2.662712574005127, loss_cls: 1.891167402267456
100/416 loss: 4.774321556091309, loss_lm: 2.84567928314209, loss_cls: 1.9286421537399292
110/416 loss: 4.948434829711914, loss_lm: 3.173771381378174, loss_cls: 1.7746633291244507
120/416 loss: 4.557326316833496, loss_lm: 2.4996955394744873, loss_cls: 2.057630777359009
130/416 loss: 4.678722381591797, loss_lm: 2.7124786376953125, loss_cls: 1.9662439823150635
140/416 loss: 4.566200256347656, loss_lm: 2.82698917388916, loss_cls: 1.7392113208770752
150/416 loss: 3.998875141143799, loss_lm: 2.4895834922790527, loss_cls: 1.5092917680740356
160/416 loss: 4.815560340881348, loss_lm: 2.747995138168335, loss_cls: 2.0675652027130127
170/416 loss: 4.168334484100342, loss_lm: 2.4973278045654297, loss_cls: 1.6710065603256226
180/416 loss: 3.9678449630737305, loss_lm: 2.0975654125213623, loss_cls: 1.8702795505523682
190/416 loss: 4.730053901672363, loss_lm: 2.6678805351257324, loss_cls: 2.06217360496521
200/416 loss: 4.094378471374512, loss_lm: 2.5540897846221924, loss_cls: 1.5402889251708984
210/416 loss: 4.555202007293701, loss_lm: 2.5231010913848877, loss_cls: 2.0321009159088135
220/416 loss: 4.906412124633789, loss_lm: 2.9253664016723633, loss_cls: 1.9810456037521362
230/416 loss: 4.3206682205200195, loss_lm: 2.495553493499756, loss_cls: 1.8251146078109741
240/416 loss: 5.0207014083862305, loss_lm: 2.8180594444274902, loss_cls: 2.2026419639587402
250/416 loss: 5.000391960144043, loss_lm: 2.824089527130127, loss_cls: 2.176302433013916
260/416 loss: 5.096595764160156, loss_lm: 2.883279323577881, loss_cls: 2.2133162021636963
270/416 loss: 5.308890342712402, loss_lm: 2.895444869995117, loss_cls: 2.4134457111358643
280/416 loss: 4.890342712402344, loss_lm: 2.4918131828308105, loss_cls: 2.398529291152954
290/416 loss: 4.84847354888916, loss_lm: 2.4241223335266113, loss_cls: 2.424351453781128
300/416 loss: 4.466952800750732, loss_lm: 2.491610050201416, loss_cls: 1.975342869758606
310/416 loss: 4.596144199371338, loss_lm: 2.5863935947418213, loss_cls: 2.0097506046295166
320/416 loss: 4.4329352378845215, loss_lm: 2.655862808227539, loss_cls: 1.7770723104476929
330/416 loss: 5.9390869140625, loss_lm: 3.022019863128662, loss_cls: 2.917067289352417
340/416 loss: 4.993181228637695, loss_lm: 2.7992987632751465, loss_cls: 2.1938822269439697
350/416 loss: 4.285186767578125, loss_lm: 2.4261326789855957, loss_cls: 1.8590539693832397
360/416 loss: 4.915379524230957, loss_lm: 2.7367019653320312, loss_cls: 2.1786773204803467
370/416 loss: 4.648312568664551, loss_lm: 2.5204594135284424, loss_cls: 2.1278533935546875
380/416 loss: 5.451061248779297, loss_lm: 3.0760135650634766, loss_cls: 2.3750476837158203
390/416 loss: 4.7225542068481445, loss_lm: 2.8270721435546875, loss_cls: 1.895481824874878
400/416 loss: 4.717528343200684, loss_lm: 2.6645638942718506, loss_cls: 2.052964448928833
410/416 loss: 4.645981311798096, loss_lm: 2.983184337615967, loss_cls: 1.662796974182129
0/416 loss: 4.691206932067871, loss_lm: 2.570070743560791, loss_cls: 2.121135950088501
10/416 loss: 4.719614028930664, loss_lm: 2.913069725036621, loss_cls: 1.806544303894043
20/416 loss: 4.310791492462158, loss_lm: 2.63918399810791, loss_cls: 1.6716073751449585
30/416 loss: 4.962362289428711, loss_lm: 2.7243804931640625, loss_cls: 2.2379815578460693
40/416 loss: 4.579860687255859, loss_lm: 2.4566421508789062, loss_cls: 2.1232187747955322
50/416 loss: 3.746274471282959, loss_lm: 2.160351514816284, loss_cls: 1.5859230756759644
60/416 loss: 4.5291948318481445, loss_lm: 2.361551523208618, loss_cls: 2.1676433086395264
70/416 loss: 4.674279689788818, loss_lm: 2.677647829055786, loss_cls: 1.9966317415237427
80/416 loss: 4.866038799285889, loss_lm: 2.8445868492126465, loss_cls: 2.021451950073242
90/416 loss: 4.351030349731445, loss_lm: 2.416233777999878, loss_cls: 1.934796690940857
100/416 loss: 4.649621963500977, loss_lm: 2.542409896850586, loss_cls: 2.1072118282318115
110/416 loss: 4.203163146972656, loss_lm: 2.5141143798828125, loss_cls: 1.6890488862991333
120/416 loss: 4.127063751220703, loss_lm: 2.4314422607421875, loss_cls: 1.6956212520599365
130/416 loss: 5.5180864334106445, loss_lm: 2.6323394775390625, loss_cls: 2.885747194290161
140/416 loss: 4.783971786499023, loss_lm: 2.730501174926758, loss_cls: 2.0534706115722656
150/416 loss: 4.733522891998291, loss_lm: 2.5689857006073, loss_cls: 2.164537191390991
160/416 loss: 4.399243354797363, loss_lm: 2.3305883407592773, loss_cls: 2.068655252456665
170/416 loss: 4.217145919799805, loss_lm: 2.4007914066314697, loss_cls: 1.8163546323776245
180/416 loss: 4.664335250854492, loss_lm: 2.5803942680358887, loss_cls: 2.0839412212371826
190/416 loss: 4.359612464904785, loss_lm: 2.4403417110443115, loss_cls: 1.919270634651184
200/416 loss: 4.820734977722168, loss_lm: 2.5775654315948486, loss_cls: 2.2431697845458984
210/416 loss: 4.058637619018555, loss_lm: 2.603555202484131, loss_cls: 1.4550821781158447
220/416 loss: 4.460745811462402, loss_lm: 2.2761502265930176, loss_cls: 2.1845955848693848
230/416 loss: 5.340713024139404, loss_lm: 3.069489002227783, loss_cls: 2.271224021911621
240/416 loss: 3.657219886779785, loss_lm: 2.2536654472351074, loss_cls: 1.4035543203353882
250/416 loss: 4.403294086456299, loss_lm: 2.4517383575439453, loss_cls: 1.951555848121643
260/416 loss: 4.0041656494140625, loss_lm: 2.294079542160034, loss_cls: 1.7100861072540283
270/416 loss: 4.18975305557251, loss_lm: 2.428389072418213, loss_cls: 1.7613641023635864
280/416 loss: 5.425533294677734, loss_lm: 2.768641471862793, loss_cls: 2.6568915843963623
290/416 loss: 4.5574846267700195, loss_lm: 2.7066421508789062, loss_cls: 1.8508427143096924
300/416 loss: 4.665032386779785, loss_lm: 2.6906538009643555, loss_cls: 1.9743783473968506
310/416 loss: 4.489562034606934, loss_lm: 2.7082481384277344, loss_cls: 1.7813136577606201
320/416 loss: 4.455827236175537, loss_lm: 2.540428400039673, loss_cls: 1.9153987169265747
330/416 loss: 4.073019981384277, loss_lm: 2.213931083679199, loss_cls: 1.859088659286499
340/416 loss: 4.865015506744385, loss_lm: 2.571200370788574, loss_cls: 2.2938151359558105
350/416 loss: 4.093817710876465, loss_lm: 2.2775888442993164, loss_cls: 1.8162286281585693
360/416 loss: 4.061633586883545, loss_lm: 2.369119167327881, loss_cls: 1.6925143003463745
370/416 loss: 4.5464911460876465, loss_lm: 2.49296236038208, loss_cls: 2.0535287857055664
380/416 loss: 4.890279769897461, loss_lm: 2.5850982666015625, loss_cls: 2.3051817417144775
390/416 loss: 4.557212829589844, loss_lm: 2.4961185455322266, loss_cls: 2.061094284057617
400/416 loss: 4.226772308349609, loss_lm: 2.428386926651001, loss_cls: 1.7983856201171875
410/416 loss: 4.192193031311035, loss_lm: 2.202310562133789, loss_cls: 1.9898827075958252
0/416 loss: 4.995482444763184, loss_lm: 2.796931505203247, loss_cls: 2.1985511779785156
10/416 loss: 4.923513889312744, loss_lm: 2.695453405380249, loss_cls: 2.228060483932495
20/416 loss: 4.138832092285156, loss_lm: 2.3828163146972656, loss_cls: 1.756015658378601
30/416 loss: 4.683233261108398, loss_lm: 2.5314598083496094, loss_cls: 2.151773452758789
40/416 loss: 4.011140823364258, loss_lm: 2.336775779724121, loss_cls: 1.6743649244308472
50/416 loss: 4.489959716796875, loss_lm: 2.4037742614746094, loss_cls: 2.0861854553222656
60/416 loss: 4.500150680541992, loss_lm: 2.701162099838257, loss_cls: 1.7989885807037354
70/416 loss: 4.835677146911621, loss_lm: 2.697551727294922, loss_cls: 2.1381256580352783
80/416 loss: 4.561148643493652, loss_lm: 2.681312084197998, loss_cls: 1.8798363208770752
90/416 loss: 4.0760579109191895, loss_lm: 2.533938407897949, loss_cls: 1.5421193838119507
100/416 loss: 4.012758255004883, loss_lm: 2.4613089561462402, loss_cls: 1.551449179649353
110/416 loss: 4.882274150848389, loss_lm: 2.3795480728149414, loss_cls: 2.5027260780334473
120/416 loss: 4.013050556182861, loss_lm: 2.3689491748809814, loss_cls: 1.6441013813018799
130/416 loss: 3.7929229736328125, loss_lm: 2.2980003356933594, loss_cls: 1.4949226379394531
140/416 loss: 4.474438667297363, loss_lm: 2.3222546577453613, loss_cls: 2.152184247970581
150/416 loss: 4.368502616882324, loss_lm: 2.4616849422454834, loss_cls: 1.9068177938461304
160/416 loss: 4.513421535491943, loss_lm: 2.5648021697998047, loss_cls: 1.9486194849014282
170/416 loss: 4.324506759643555, loss_lm: 2.625035285949707, loss_cls: 1.6994712352752686
180/416 loss: 4.123650550842285, loss_lm: 2.1329383850097656, loss_cls: 1.99071204662323
190/416 loss: 3.9578418731689453, loss_lm: 2.1818277835845947, loss_cls: 1.7760140895843506
200/416 loss: 4.088586330413818, loss_lm: 1.9511359930038452, loss_cls: 2.1374504566192627
210/416 loss: 4.637073993682861, loss_lm: 2.18546199798584, loss_cls: 2.4516119956970215
220/416 loss: 4.556191444396973, loss_lm: 2.4001150131225586, loss_cls: 2.156076669692993
230/416 loss: 4.226165771484375, loss_lm: 2.3723769187927246, loss_cls: 1.85378897190094
240/416 loss: 4.714381217956543, loss_lm: 2.431729316711426, loss_cls: 2.282651662826538
250/416 loss: 3.743335485458374, loss_lm: 2.1034018993377686, loss_cls: 1.6399335861206055
260/416 loss: 4.534605979919434, loss_lm: 2.7552337646484375, loss_cls: 1.7793720960617065
270/416 loss: 4.780289649963379, loss_lm: 2.5328822135925293, loss_cls: 2.2474076747894287
280/416 loss: 4.8984222412109375, loss_lm: 2.7517471313476562, loss_cls: 2.1466753482818604
290/416 loss: 3.975419759750366, loss_lm: 2.313429594039917, loss_cls: 1.6619901657104492
300/416 loss: 4.05780553817749, loss_lm: 2.4407224655151367, loss_cls: 1.617083191871643
310/416 loss: 4.689540863037109, loss_lm: 2.6055641174316406, loss_cls: 2.0839765071868896
320/416 loss: 4.913318634033203, loss_lm: 2.5462284088134766, loss_cls: 2.3670904636383057
330/416 loss: 4.435739517211914, loss_lm: 2.48978853225708, loss_cls: 1.945951223373413
340/416 loss: 4.159683704376221, loss_lm: 2.4137840270996094, loss_cls: 1.7458995580673218
350/416 loss: 4.1052632331848145, loss_lm: 2.4978749752044678, loss_cls: 1.6073883771896362
360/416 loss: 4.732686996459961, loss_lm: 2.576772451400757, loss_cls: 2.155914545059204
370/416 loss: 4.531188011169434, loss_lm: 2.476691246032715, loss_cls: 2.054497003555298
380/416 loss: 5.055480003356934, loss_lm: 2.7629036903381348, loss_cls: 2.2925760746002197
390/416 loss: 4.687323093414307, loss_lm: 2.6564931869506836, loss_cls: 2.030829906463623
400/416 loss: 4.510632038116455, loss_lm: 2.47434139251709, loss_cls: 2.0362906455993652
410/416 loss: 4.613626956939697, loss_lm: 2.339184522628784, loss_cls: 2.274442434310913
0/416 loss: 4.519491195678711, loss_lm: 2.65401029586792, loss_cls: 1.8654807806015015
10/416 loss: 4.426059246063232, loss_lm: 2.068652629852295, loss_cls: 2.3574066162109375
20/416 loss: 4.276886940002441, loss_lm: 2.3739683628082275, loss_cls: 1.902918815612793
30/416 loss: 4.349485397338867, loss_lm: 2.3354930877685547, loss_cls: 2.0139923095703125
40/416 loss: 5.460976600646973, loss_lm: 2.8243985176086426, loss_cls: 2.636578321456909
50/416 loss: 4.077590465545654, loss_lm: 2.2800121307373047, loss_cls: 1.79757821559906
60/416 loss: 5.258591175079346, loss_lm: 2.5538015365600586, loss_cls: 2.704789638519287
70/416 loss: 4.45747184753418, loss_lm: 2.527996301651001, loss_cls: 1.9294756650924683
80/416 loss: 4.252597332000732, loss_lm: 2.2549197673797607, loss_cls: 1.9976775646209717
90/416 loss: 4.466744422912598, loss_lm: 2.4817819595336914, loss_cls: 1.9849625825881958
100/416 loss: 4.298969745635986, loss_lm: 2.3054404258728027, loss_cls: 1.9935294389724731
110/416 loss: 4.689609050750732, loss_lm: 2.8184595108032227, loss_cls: 1.8711494207382202
120/416 loss: 3.8117876052856445, loss_lm: 2.2357561588287354, loss_cls: 1.5760315656661987
130/416 loss: 4.609682083129883, loss_lm: 2.5009243488311768, loss_cls: 2.108757495880127
140/416 loss: 4.611135482788086, loss_lm: 2.616812229156494, loss_cls: 1.9943230152130127
150/416 loss: 4.75945520401001, loss_lm: 2.629366397857666, loss_cls: 2.1300888061523438
160/416 loss: 4.5185675621032715, loss_lm: 2.1613969802856445, loss_cls: 2.357170581817627
170/416 loss: 5.284473419189453, loss_lm: 2.7063779830932617, loss_cls: 2.5780956745147705
180/416 loss: 4.758520126342773, loss_lm: 2.5236897468566895, loss_cls: 2.234830141067505
190/416 loss: 4.009181976318359, loss_lm: 2.2619948387145996, loss_cls: 1.7471870183944702
200/416 loss: 4.066664218902588, loss_lm: 2.541144847869873, loss_cls: 1.5255192518234253
210/416 loss: 4.723268508911133, loss_lm: 2.617103338241577, loss_cls: 2.1061649322509766
220/416 loss: 4.177268981933594, loss_lm: 2.3118185997009277, loss_cls: 1.8654502630233765
230/416 loss: 4.679958343505859, loss_lm: 2.539954662322998, loss_cls: 2.1400034427642822
240/416 loss: 4.788692951202393, loss_lm: 2.494389057159424, loss_cls: 2.2943038940429688
250/416 loss: 4.325578689575195, loss_lm: 2.29398775100708, loss_cls: 2.0315909385681152
260/416 loss: 4.067709922790527, loss_lm: 2.4713046550750732, loss_cls: 1.5964051485061646
270/416 loss: 4.006023406982422, loss_lm: 2.301180124282837, loss_cls: 1.7048431634902954
280/416 loss: 4.262665748596191, loss_lm: 2.3204612731933594, loss_cls: 1.9422045946121216
290/416 loss: 4.213662147521973, loss_lm: 2.191194772720337, loss_cls: 2.0224671363830566
300/416 loss: 4.0906524658203125, loss_lm: 2.367129325866699, loss_cls: 1.7235231399536133
310/416 loss: 4.095879554748535, loss_lm: 2.1817548274993896, loss_cls: 1.914124608039856
320/416 loss: 3.967757225036621, loss_lm: 2.2840964794158936, loss_cls: 1.6836607456207275
330/416 loss: 4.12638521194458, loss_lm: 2.264054536819458, loss_cls: 1.8623307943344116
340/416 loss: 5.14516544342041, loss_lm: 2.627366542816162, loss_cls: 2.517798900604248
350/416 loss: 4.378331184387207, loss_lm: 2.52390718460083, loss_cls: 1.8544238805770874
360/416 loss: 4.424139022827148, loss_lm: 2.4431018829345703, loss_cls: 1.9810373783111572
370/416 loss: 3.9129791259765625, loss_lm: 2.199376106262207, loss_cls: 1.713603138923645
380/416 loss: 3.934251308441162, loss_lm: 2.2907896041870117, loss_cls: 1.6434615850448608
390/416 loss: 4.349421501159668, loss_lm: 2.50873064994812, loss_cls: 1.8406908512115479
400/416 loss: 4.374023914337158, loss_lm: 2.6973743438720703, loss_cls: 1.6766496896743774
410/416 loss: 4.702443599700928, loss_lm: 2.310516595840454, loss_cls: 2.3919270038604736
Saving current best to results/promptmrg/experiment_results/custom_iu_model\custom_iu_model.pth
